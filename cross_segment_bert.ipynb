{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "#from text_manipulation import word_model\n",
    "#from text_manipulation import extract_sentence_words\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from pathlib2 import Path\n",
    "import re\n",
    "import os\n",
    "import math\n",
    "import gensim\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from transformers import AdamW\n",
    "from transformers import BertModel, BertTokenizer\n",
    "\n",
    "bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "# Load the BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "MAX_TOKENS = 200\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#logger = utils.setup_logger(__name__, 'train.log')\n",
    "missing_stop_words = set(['of', 'a', 'and', 'to'])\n",
    "section_delimiter = \"========\"\n",
    "segment_seperator = \"========\"\n",
    "\n",
    "def get_list_token():\n",
    "    return \"***LIST***\"\n",
    "\n",
    "def get_formula_token():\n",
    "    return \"***formula***\"\n",
    "\n",
    "def get_codesnipet_token():\n",
    "    return \"***codice***\"\n",
    "\n",
    "def get_special_tokens():\n",
    "    special_tokens = []\n",
    "    special_tokens.append(get_list_token())\n",
    "    special_tokens.append(get_formula_token())\n",
    "    special_tokens.append(get_codesnipet_token())\n",
    "    return special_tokens\n",
    "\n",
    "def get_seperator_foramt(levels = None):\n",
    "    level_format = '\\d' if levels == None else '['+ str(levels[0]) + '-' + str(levels[1]) + ']'\n",
    "    seperator_fromat = segment_seperator + ',' + level_format + \",.*?\\.\"\n",
    "    return seperator_fromat\n",
    "\n",
    "words_tokenizer = None\n",
    "def get_words_tokenizer():\n",
    "    global words_tokenizer\n",
    "\n",
    "    if words_tokenizer:\n",
    "        return words_tokenizer\n",
    "\n",
    "    words_tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    return words_tokenizer\n",
    "\n",
    "def extract_sentence_words(sentence, remove_missing_emb_words = False, remove_special_tokens = False):\n",
    "    if (remove_special_tokens):\n",
    "        for token in get_special_tokens():\n",
    "            # Can't do on sentence words because tokenizer delete '***' of tokens.\n",
    "            sentence = sentence.replace(token, \"\")\n",
    "    tokenizer = get_words_tokenizer()\n",
    "    sentence_words = tokenizer.tokenize(sentence)\n",
    "    if remove_missing_emb_words:\n",
    "        sentence_words = [w for w in sentence_words if w not in missing_stop_words]\n",
    "\n",
    "    return sentence_words\n",
    "\n",
    "\n",
    "def word_model(word, model):http://127.0.0.1:8990/?token=bf09ebab079e0fa44f92e9781373743116328722bca9e067\n",
    "    if model is None:\n",
    "        return np.random.randn(1, 300)\n",
    "    else:\n",
    "        if word in model:\n",
    "            return model[word].reshape(1, 300)\n",
    "        else:\n",
    "            #print ('Word missing w2v: ' + word)\n",
    "            return model['UNK'].reshape(1, 300)\n",
    "\n",
    "def get_files(path):\n",
    "    all_objects = Path(path).glob('**/*')\n",
    "    files = [str(p) for p in all_objects if p.is_file()]\n",
    "    return files\n",
    "\n",
    "\n",
    "def get_cache_path(wiki_folder):\n",
    "    cache_file_path = wiki_folder / 'paths_cache'\n",
    "    return cache_file_path\n",
    "\n",
    "\n",
    "def cache_wiki_filenames(wiki_folder):\n",
    "    files = Path(wiki_folder).glob('*/*/*/*')\n",
    "    cache_file_path = get_cache_path(wiki_folder)\n",
    "\n",
    "    with cache_file_path.open('w+') as f:\n",
    "        for file in files:\n",
    "            f.write(file + u'\\n')\n",
    "\n",
    "\n",
    "def clean_section(section):\n",
    "    cleaned_section = section.strip('\\n')\n",
    "    return cleaned_section\n",
    "\n",
    "\n",
    "def get_scections_from_text(txt, high_granularity=True):\n",
    "    sections_to_keep_pattern = get_seperator_foramt() if high_granularity else get_seperator_foramt(\n",
    "        (1, 2))\n",
    "    if not high_granularity:\n",
    "        # if low granularity required we should flatten segments within segemnt level 2\n",
    "        pattern_to_ommit = get_seperator_foramt((3, 999))\n",
    "        txt = re.sub(pattern_to_ommit, \"\", txt)\n",
    "\n",
    "        #delete empty lines after re.sub()\n",
    "        sentences = [s for s in txt.strip().split(\"\\n\") if len(s) > 0 and s != \"\\n\"]\n",
    "        txt = '\\n'.join(sentences).strip('\\n')\n",
    "\n",
    "\n",
    "    all_sections = re.split(sections_to_keep_pattern, txt)\n",
    "    non_empty_sections = [s for s in all_sections if len(s) > 0]\n",
    "\n",
    "    return non_empty_sections\n",
    "\n",
    "\n",
    "def get_sections(path, high_granularity=True):\n",
    "    file = open(str(path), \"r\")\n",
    "    raw_content = file.read()\n",
    "    file.close()\n",
    "\n",
    "    clean_txt = raw_content.strip()\n",
    "\n",
    "    sections = [clean_section(s) for s in get_scections_from_text(clean_txt, high_granularity)]\n",
    "\n",
    "    return sections\n",
    "\n",
    "def read_wiki_file(path, n_context_sent = 1, remove_preface_segment=True, high_granularity=True):\n",
    "    data = []\n",
    "    targets = []\n",
    "    all_sections = get_sections(path, high_granularity)\n",
    "    required_sections = all_sections[1:] if remove_preface_segment and len(all_sections) > 0 else all_sections\n",
    "    required_non_empty_sections = [section for section in required_sections if len(section) > 0 and section != \"\\n\"]\n",
    "\n",
    "    list_sentence = get_list_token() + \".\"\n",
    "    final_sentences = []\n",
    "    label = []\n",
    "    for section_ind in range(len(required_non_empty_sections)):\n",
    "        sentences_ = required_non_empty_sections[section_ind].split('\\n')\n",
    "        sentences = [x for x in sentences_ if x != list_sentence]\n",
    "        if sentences:\n",
    "            for sentence in sentences[:-1]:\n",
    "                final_sentences.append(sentence)\n",
    "                label.append(0)\n",
    "            final_sentences.append(sentences[-1])\n",
    "            label.append(1)\n",
    "    \n",
    "    if len(final_sentences)>n_context_sent:\n",
    "        for sent_ind in range(n_context_sent,len(final_sentences)):\n",
    "            prev_context = final_sentences[sent_ind-n_context_sent:sent_ind]\n",
    "            after_context = final_sentences[sent_ind: min(len(final_sentences),sent_ind+n_context_sent)]\n",
    "            \n",
    "            prev_context = \" \".join(prev_context)\n",
    "            after_context = \" \".join(after_context)\n",
    "            data.append([prev_context, after_context])\n",
    "            targets.append(label[sent_ind-1])\n",
    "\n",
    "    return data, targets, path\n",
    "\n",
    "\n",
    "class WikipediaDataSet(Dataset):\n",
    "    def __init__(self, root,n_context_sent = 1, train=True, manifesto=False, folder=False, high_granularity=False):\n",
    "\n",
    "        if (manifesto):\n",
    "            self.textfiles = list(Path(root).glob('*'))\n",
    "        else:\n",
    "            if (folder):\n",
    "                self.textfiles = get_files(root)\n",
    "            else:\n",
    "                root_path = Path(root)\n",
    "                print(root_path)\n",
    "                cache_path = get_cache_path(root_path)\n",
    "                print(cache_path)\n",
    "                if not cache_path.exists():\n",
    "                    print(\"not_exist\")\n",
    "                    cache_wiki_filenames(root_path)\n",
    "                self.textfiles = cache_path.read_text().splitlines()\n",
    "\n",
    "        if len(self.textfiles) == 0:\n",
    "            raise RuntimeError('Found 0 images in subfolders of: {}'.format(root))\n",
    "        self.train = train\n",
    "        self.root = root\n",
    "        self.high_granularity = high_granularity\n",
    "        self.n_context_sent = n_context_sent\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.textfiles[index]\n",
    "\n",
    "        return read_wiki_file(Path(path),n_context_sent = 2,high_granularity=self.high_granularity)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.textfiles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def collate_fn(batch):\n",
    "    batched_data = []\n",
    "    batched_targets = []\n",
    "    batched_paths = []\n",
    "\n",
    "    window_size = 1\n",
    "    before_sentence_count = int(math.ceil(float(window_size - 1) /2))\n",
    "    after_sentence_count = window_size - before_sentence_count - 1\n",
    "    max_tokens = 100\n",
    "    for data, targets, path in batch:\n",
    "        try:\n",
    "            for i in range(len(data)):\n",
    "                temp = len(data[i][0].split())+len(data[i][1].split())\n",
    "                if max_tokens < temp:\n",
    "                    max_tokens = temp\n",
    "                batched_data.append(data[i])\n",
    "                batched_targets.append(targets[i])\n",
    "                batched_paths.append(path)\n",
    "        except Exception as e:\n",
    "            logger.info('Exception \"%s\" in file: \"%s\"', e, path)\n",
    "            logger.debug('Exception!', exc_info=True)\n",
    "            continue\n",
    "    \n",
    "    max_tokens = min(MAX_TOKENS, max_tokens)\n",
    "    tokens = tokenizer(\n",
    "                    batched_data,\n",
    "                    padding = True,\n",
    "                    max_length = max_tokens,\n",
    "                    truncation=True)\n",
    "    '''seq = torch.tensor(tokens['input_ids'])\n",
    "    mask = torch.tensor(tokens['attention_mask'])\n",
    "    y = torch.tensor(batched_targets)'''\n",
    "        \n",
    "    return tokens['input_ids'], tokens['attention_mask'], batched_targets, batched_paths\n",
    "\n",
    "\n",
    "dataset_path = \"/home/aakash/amagi/data/wiki/wiki_727\"\n",
    "dataset = WikipediaDataSet(dataset_path+'/dev', high_granularity=False)\n",
    "dl = DataLoader(dataset, batch_size=12, collate_fn = collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent,target,path = dataset.__getitem__(1)\n",
    "print(sent[0][0])\n",
    "print(len(sent),len(target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(sent[0]))\n",
    "print(sent[1])\n",
    "print(sent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenizer(\n",
    "                    sent,\n",
    "                    padding = True,\n",
    "                    max_length = 200,\n",
    "                    truncation=True)\n",
    "print(tokenizer.decode(tokens['input_ids'][2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dl))\n",
    "_,batch = next(enumerate(dl))\n",
    "print(len(batch))\n",
    "input_, mask, targets, paths = batch\n",
    "print(len(input_),len(mask),len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in bert.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder_Classifier(nn.Module):\n",
    "    def __init__(self, bert, n_classes):\n",
    "        super(Encoder_Classifier, self).__init__()\n",
    "        self.bert = bert\n",
    "\n",
    "        # dropout layer\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "\n",
    "        # relu activation function\n",
    "        self.relu =  nn.ReLU()\n",
    "\n",
    "        # dense layer 1\n",
    "        self.fc1 = nn.Linear(768,n_classes)\n",
    "        \n",
    "        #softmax activation function\n",
    "        self.softmax = nn.LogSoftmax(dim=1)\n",
    "\n",
    "    #define the forward pass\n",
    "    def forward(self, sent_id, mask):\n",
    "        #pass the inputs to the model  \n",
    "        _, cls_hs = self.bert(sent_id, attention_mask=mask, return_dict=False)\n",
    "\n",
    "\n",
    "        x = self.fc1(cls_hs) \n",
    "\n",
    "        # apply softmax activation\n",
    "        x = self.softmax(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_dataloader):\n",
    "    model.train()\n",
    "\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    '''\n",
    "    # empty list to save model predictions\n",
    "    total_preds=[]'''\n",
    "  \n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(train_dataloader):\n",
    "\n",
    "        # push the batch to gpu\n",
    "        #batch = [r.to(device) for r in batch]\n",
    "\n",
    "        ###### for  labeled data, computing cross entropy   #########\n",
    "        sent_id, mask, labels = torch.tensor(batch[0]).to(device),torch.tensor(batch[1]).to(device),torch.tensor(batch[2]).to(device)\n",
    "\n",
    "        model.zero_grad()        \n",
    "        preds = model(sent_id, mask)\n",
    "        loss = CELoss(preds, labels)\n",
    "\n",
    "        # backward pass to calculate the gradients\n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "        # update parameters\n",
    "        optimizer.step()\n",
    "        \n",
    "        #torch.cuda.empty_cache()\n",
    "        # add on to the total loss\n",
    "        loss_item = loss.item()\n",
    "        total_loss += loss_item\n",
    "\n",
    "        # progress update after every 100 batches.\n",
    "        if step % 100 == 0 and not step == 0:\n",
    "            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))\n",
    "            print(\"loss\",loss_item)\n",
    "\n",
    "         \n",
    "\n",
    "        '''\n",
    "        # model predictions are stored on GPU. So, push it to CPU\n",
    "        preds=preds.detach().cpu().numpy()\n",
    "\n",
    "        # append the model predictions\n",
    "        total_preds.append(preds)'''\n",
    "\n",
    "    # compute the training loss of the epoch\n",
    "    avg_loss = total_loss / len(train_dataloader)\n",
    "    # predictions are in the form of (no. of batches, size of batch, no. of classes).\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    #total_preds  = np.concatenate(total_preds, axis=0)\n",
    "\n",
    "    #returns the loss and predictions\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def evaluate(dev_dataloader):\n",
    "  \n",
    "    print(\"\\nEvaluating...\")\n",
    "  \n",
    "    # deactivate dropout layers\n",
    "    model.eval()\n",
    "    total_loss, total_accuracy = 0, 0\n",
    "    total_preds = [[None,None]]\n",
    "    count=0\n",
    "    curr_examples = 0\n",
    "\n",
    "    # iterate over batches\n",
    "    for step,batch in enumerate(dev_dataloader):\n",
    "    \n",
    "        # Progress update every 50 batches.\n",
    "        if step % 1 == 0 and not step == 0:\n",
    "      \n",
    "            # Report progress.\n",
    "            print('  Batch {:>5,}  of  {:>5,} accuracy {}.'.format(step, len(dev_dataloader), count/curr_examples))\n",
    "            temp = np.delete(total_preds,0,0)\n",
    "            print(\"F1 score {}\".format(f1_score(list(temp[:,0]),list(temp[:,1]),average=\"macro\")))\n",
    "        # push the batch to gpu\n",
    "        #batch = [t.to(device) for t in batch]\n",
    "    \n",
    "\n",
    "        sent_id, mask, labels = torch.tensor(batch[0]).to(device),torch.tensor(batch[1]).to(device),torch.tensor(batch[2]).to(device)\n",
    "        curr_examples += len(sent_id)\n",
    "        # deactivate autograd\n",
    "        with torch.no_grad():\n",
    "      \n",
    "        # model predictions\n",
    "            preds = model(sent_id, mask)\n",
    "\n",
    "            # compute the validation loss between actual and predicted values\n",
    "            loss = CELoss(preds,labels)\n",
    "\n",
    "            total_loss = total_loss + loss.item()\n",
    "            preds = preds.detach().cpu().numpy()\n",
    "\n",
    "            true_class = np.expand_dims(np.argmax(preds,axis=1),axis=0)\n",
    "            labels = np.expand_dims(labels.detach().cpu().numpy(), axis=0)\n",
    "            temp = np.concatenate((labels,true_class),axis=0).T\n",
    "            \n",
    "            total_preds = np.concatenate((total_preds,temp),axis=0)\n",
    "\n",
    "            \n",
    "            for myvar in range(len(labels[0])):\n",
    "                if labels[0][myvar]== true_class[0][myvar]:\n",
    "                    count+=1\n",
    "\n",
    "    # compute the validation loss of the epoch\n",
    "    avg_loss = total_loss / len(dev_dataloader) \n",
    "\n",
    "    # reshape the predictions in form of (number of samples, no. of classes)\n",
    "    total_preds  = np.concatenate(total_preds, axis=0)\n",
    "    print(\"Validation accuracy\",count/len(dev_data))\n",
    "\n",
    "    return avg_loss, count/len(dev_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_valid_loss = float('inf')\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "accuracy_list = []\n",
    "learning_rate = .0005\n",
    "freezed_epochs = 1\n",
    "unfreezed_epochs = 1\n",
    "n_classes = 2\n",
    "n_layer_unfreeze = 2\n",
    "weight = .3\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "CELoss = nn.CrossEntropyLoss()\n",
    "model = Encoder_Classifier(bert, n_classes).to(device)\n",
    "optimizer = AdamW(model.parameters(),lr = learning_rate)\n",
    "print(device)\n",
    "\n",
    "dataset_path = \"/home/aakash/amagi/data/wiki/wiki_727\"\n",
    "dataset_train = WikipediaDataSet(dataset_path+'/dev', high_granularity=False)\n",
    "train_dataloader = DataLoader(dataset_train, batch_size=2, collate_fn = collate_fn, shuffle=True)\n",
    "\n",
    "dataset_dev = WikipediaDataSet(dataset_path+'/test', high_granularity=False)\n",
    "dev_dataloader = DataLoader(dataset_dev, batch_size=2, collate_fn = collate_fn, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(freezed_epochs):\n",
    "     \n",
    "    print('\\n Epoch {:} / {:}'.format(epoch + 1, freezed_epochs))\n",
    "    \n",
    "    #train model\n",
    "    train_loss = train(train_dataloader)\n",
    "    \n",
    "    #evaluate model\n",
    "    valid_loss, accuracy = evaluate(dev_dataloader)\n",
    "    \n",
    "    #save the best model\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "    \n",
    "    # append training and validation loss\n",
    "    train_losses.append(train_loss)\n",
    "    valid_losses.append(valid_loss)\n",
    "    accuracy_list.append(accuracy)\n",
    "\n",
    "    print(f'\\nTraining Loss: {train_loss[0]:.3f}')\n",
    "    print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), '/home/aakash/amagi/data/wiki/wiki_727/saved_weights.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### unfreezing layers #######\n",
    "for iter in range(n_layer_unfreeze):\n",
    "    print(str(iter+1)+\" unfreeze\")\n",
    "    for param in model.bert.encoder.layer._modules[str(11-iter)].parameters():\n",
    "        param.requires_grad=True\n",
    "    for epoch in range(unfreezed_epochs):\n",
    "     \n",
    "        print('\\n Epoch {:} / {:}'.format(epoch+1 , ))\n",
    "        \n",
    "        #train model\n",
    "        train_loss = train(train_dataloader)\n",
    "    \n",
    "        #evaluate model\n",
    "        valid_loss, accuracy = evaluate(dev_dataloader)\n",
    "        \n",
    "        #save the best model\n",
    "        if valid_loss < best_valid_loss:\n",
    "            best_valid_loss = valid_loss\n",
    "            torch.save(model.state_dict(), 'saved_weights.pt')\n",
    "        \n",
    "        # append training and validation loss\n",
    "        train_losses.append(train_loss)\n",
    "        valid_losses.append(valid_loss)\n",
    "        accuracy_list.append(accuracy)\n",
    "        \n",
    "        print(f'\\nTraining Loss: {train_loss[0]:.3f}')\n",
    "        print(f'Validation Loss: {valid_loss:.3f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
